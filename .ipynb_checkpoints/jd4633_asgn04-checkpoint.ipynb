{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0864fdfc",
   "metadata": {},
   "source": [
    "## Task 1: Load the KDD99Cup data and prepare to train DOS vs non-DOS\n",
    "We have downloaded the the KDD99Cup data and have it in the local directory as \"kddcup.data\". We have also created a file called \"kddcup.headers\" in the local directory, which is a space delimited list of the header names for the features. We load the data into a DataFrame. We create a target array which is 1 for DOS attacks and 0 for all other records.\n",
    "\n",
    "Since SVM is very computationally expensive, we reduce our data set to 100,000 DOS records and 100,000 non-DOS records. We then use OneHotEncoder to encode the categorical columns, and finally scale our columns using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce747a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kddcup data. This was downloaded and placed in the same directory as the notebook.\n",
    "# For header names, use file we created in the local directory called \"kddcup.headers\".\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "df = pd.read_csv('kddcup.data')\n",
    "headers = np.genfromtxt('kddcup.headers', dtype=str, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0369f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data set: (4898430, 42)\n",
      "shape of X: (4898430, 41)\n",
      "shape of y_raw: (4898430,)\n",
      "\n",
      "Count of records of each type:\n",
      "normal.: 972780\n",
      "buffer_overflow.: 30\n",
      "loadmodule.: 9\n",
      "perl.: 3\n",
      "neptune.: 1072017\n",
      "smurf.: 2807886\n",
      "guess_passwd.: 53\n",
      "pod.: 264\n",
      "teardrop.: 979\n",
      "portsweep.: 10413\n",
      "ipsweep.: 12481\n",
      "land.: 21\n",
      "ftp_write.: 8\n",
      "back.: 2203\n",
      "imap.: 12\n",
      "satan.: 15892\n",
      "phf.: 4\n",
      "nmap.: 2316\n",
      "multihop.: 7\n",
      "warezmaster.: 20\n",
      "warezclient.: 1020\n",
      "spy.: 2\n",
      "rootkit.: 10\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the original data set, for reference\n",
    "print(f\"shape of data set: {df.shape}\")\n",
    "\n",
    "# Set the feature set (X) to columns 0-40 of the data set. Set the headers of the feature set using the\n",
    "# file we imported above.\n",
    "X = df.iloc[:, 0:41]\n",
    "X.columns = headers\n",
    "print(f\"shape of X: {X.shape}\")\n",
    "\n",
    "# Create an ndarray \"y_raw\" with the target labels from column 41 of the data set. Call it \"y_raw\" as we\n",
    "# will encode it to create our target array y.\n",
    "y_raw = df.iloc[:, 41].values\n",
    "print(f\"shape of y_raw: {y_raw.shape}\")\n",
    "print()\n",
    "\n",
    "#Print the number of records of each class\n",
    "from collections import Counter\n",
    "class_counts = Counter(y_raw)\n",
    "print(\"Count of records of each type:\")\n",
    "for c in class_counts:\n",
    "    print(f\"{c}: {class_counts[c]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb452d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'icmp': 2833545, 'tcp': 1870597, 'udp': 194288})\n",
      "Counter({'ecr_i': 2811660, 'private': 1100831, 'http': 623090, 'smtp': 96554, 'other': 72653, 'domain_u': 57782, 'ftp_data': 40697, 'eco_i': 16338, 'finger': 6891, 'urp_i': 5378, 'ftp': 5214, 'telnet': 4277, 'ntp_u': 3833, 'auth': 3382, 'pop_3': 1981, 'time': 1579, 'domain': 1113, 'Z39_50': 1078, 'gopher': 1077, 'mtp': 1076, 'ssh': 1075, 'whois': 1073, 'remote_job': 1073, 'rje': 1070, 'link': 1069, 'imap4': 1069, 'ctf': 1068, 'name': 1067, 'supdup': 1060, 'echo': 1059, 'discard': 1059, 'nntp': 1059, 'uucp_path': 1057, 'netstat': 1056, 'daytime': 1056, 'systat': 1056, 'sunrpc': 1056, 'netbios_ssn': 1055, 'pop_2': 1055, 'netbios_ns': 1054, 'vmnet': 1053, 'netbios_dgm': 1052, 'sql_net': 1052, 'iso_tsap': 1052, 'shell': 1051, 'csnet_ns': 1051, 'klogin': 1050, 'hostnames': 1050, 'bgp': 1047, 'login': 1045, 'exec': 1045, 'printer': 1045, 'http_443': 1044, 'efs': 1042, 'uucp': 1041, 'ldap': 1041, 'kshell': 1040, 'nnsp': 1038, 'courier': 1021, 'IRC': 521, 'urh_i': 148, 'X11': 135, 'tim_i': 12, 'red_i': 9, 'pm_dump': 5, 'tftp_u': 3, 'harvest': 2, 'aol': 2, 'http_8001': 2, 'http_2784': 1})\n",
      "Counter({'SF': 3744327, 'S0': 869829, 'REJ': 268874, 'RSTR': 8094, 'RSTO': 5344, 'SH': 1040, 'S1': 532, 'S2': 161, 'RSTOS0': 122, 'OTH': 57, 'S3': 50})\n"
     ]
    }
   ],
   "source": [
    "# Look at the occurence of the different categories in the categorical features\n",
    "print(Counter(X['protocol_type']))\n",
    "print(Counter(X['service']))\n",
    "print(Counter(X['flag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c5a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for converting target labels to 1 (DOS) or 0 (non-DOS). We will use this function\n",
    "# to encode the target array.\n",
    "dosAttackTypes = [\"back.\", \"land.\", \"neptune.\", \"pod.\", \"smurf.\", \"teardrop.\"]\n",
    "\n",
    "def DOSEncode(x):\n",
    "    if (x in dosAttackTypes):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8deda9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 3883370, 0: 1015060})\n"
     ]
    }
   ],
   "source": [
    "# Use the Dataframe function \"applymap\" to apply our encoding function \"DOSEncode\" to convert the target\n",
    "# labels into 1's and 0's.\n",
    "labels=pd.DataFrame(data=y_raw)\n",
    "labels2 = labels.applymap(DOSEncode)\n",
    "y = labels2[0].values\n",
    "\n",
    "# Print out the counts of the encoded values.\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7371ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 3883370, 0: 1015060})\n"
     ]
    }
   ],
   "source": [
    "# The dataset is unbalanced, we have many more failure cases than success cases. Use RandomUnderSampler to \n",
    "# balance the number of records of each type.\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "# Print the shape of the original data set\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "# Use RandomUnderSampler, to take 100,000 DOS records and 100,000 non-DOS records\n",
    "# Had to limit to only that number of samples, or else SVM was not converging in a reasonable amount of time\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy={1:100000, 0:100000})\n",
    "X1, y1 = rus.fit_resample(X, y)\n",
    "\n",
    "# Print shape of resampled data set, and count of label values in the target array\n",
    "print(f'Resampled dataset shape: {X1.shape}')\n",
    "print(f'Count of label values: {Counter(y1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder on protocol_type\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a dataframe of encoded data from \"protocol_type\"\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "X2 = pd.DataFrame(enc.fit_transform(X1[['protocol_type']]))\n",
    "X2.columns = enc.get_feature_names_out(['protocol_type'])\n",
    "\n",
    "# Drop the original \"protocol_type\" column, and merge in the new encoded columns\n",
    "X3 = X1.copy()\n",
    "X3.drop(['protocol_type'], axis=1, inplace=True)\n",
    "X4 = pd.concat([X3, X2], axis=1)\n",
    "\n",
    "# Print the resultant shape\n",
    "print(X4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder on service\n",
    "\n",
    "# Create a dataframe of encoded data from \"service\"\n",
    "X5 = pd.DataFrame(enc.fit_transform(X4[['service']]))\n",
    "X5.columns = enc.get_feature_names_out(['service'])\n",
    "\n",
    "# Drop the original \"service\" column, and merge in the new encoded columns\n",
    "X6 = X4.copy()\n",
    "X6.drop(['service'], axis=1, inplace=True)\n",
    "X7 = pd.concat([X6, X5 ], axis=1)\n",
    "\n",
    "# Print the resultant shape\n",
    "print(X7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe317ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder on flag\n",
    "\n",
    "# Create a dataframe of encoded data from \"flag\"\n",
    "X8 = pd.DataFrame(enc.fit_transform(X7[['flag']]))\n",
    "X8.columns = enc.get_feature_names_out(['flag'])\n",
    "\n",
    "# Drop the original \"flag\" column, and merge in the new encoded columns\n",
    "X9 = X7.copy()\n",
    "X9.drop(['flag'], axis=1, inplace=True)\n",
    "X10 = pd.concat([X9, X8], axis=1)\n",
    "\n",
    "# Print the resultant shape\n",
    "print(X10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final feature set into X_encoded\n",
    "X_encoded = X10\n",
    "\n",
    "# View dataframe\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature set data to help with training.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Use MinMaxScaler to scale the data set\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_encoded)\n",
    "scaledArray = scaler.transform(X_encoded)\n",
    "X_scaled = pd.DataFrame(data=scaledArray, columns=X_encoded.columns)\n",
    "\n",
    "# View dataframe\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609afe6",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "We were not able to get the SVM kernels to run in a reasonable time. Therefore with permission from the professor, we are identifying 15 of the most important features and limiting our data set to those features.\n",
    "\n",
    "We select the features using sklearn's SelectKBest class. Once we have selected our features, we drop the others from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 15 best features via SelectKBest, and then store their names into the array \"features_to_keep\"\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n",
    "print(\"selecting 15 best features to use:\")\n",
    "selector = SelectKBest(chi2, k=15).fit(X_scaled, y1)\n",
    "features_to_keep = selector.get_feature_names_out()\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dataframe \"X_reduced\", which starts as a copy of our final data set from above.\n",
    "# Loop through \"X_reduced\", and drop every column which is not part of the array \"features_to_keep\".\n",
    "columnList = X_scaled.columns\n",
    "\n",
    "X_reduced = X_scaled.copy()\n",
    "for column in columnList:\n",
    "    if (column not in features_to_keep):\n",
    "        X_reduced.drop(columns=column, inplace=True)\n",
    "\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5a950",
   "metadata": {},
   "source": [
    "### Split data into Test and Train data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac14e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set (80%) and testing set (20%).\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y1, test_size=0.2, random_state=0)\n",
    "print(f\"training set shape: {X_train.shape}\")\n",
    "print(f\"testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04284fc2",
   "metadata": {},
   "source": [
    "## Task 2: Run the SVM Model with 4 Different Kernels, and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fe6e6",
   "metadata": {},
   "source": [
    "### Train and test with \"rbf\" kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RBF classifier and train with the training set.\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "svm1 = SVC(kernel='rbf', random_state=0)\n",
    "print(svm1.get_params())\n",
    "\n",
    "# Time how long it takes to train.\n",
    "start = time.time()\n",
    "svm1.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "svm1_train_time = end - start\n",
    "\n",
    "print(f\"secs to fit: {svm1_train_time:.2f}\")\n",
    "print(f\"iterations required: {svm1.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24522dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm1_train_predict = svm1.predict(X_train)\n",
    "end = time.time()\n",
    "svm1_pred_time = end - start\n",
    "\n",
    "print(f\"secs to predict (training set): {svm1_pred_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for training data set\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_train, svm1_train_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, svm1_train_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm1_test_predict = svm1.predict(X_test)\n",
    "end = time.time()\n",
    "svm1_test_time = end - start\n",
    "\n",
    "print(f\"secs to predict (test set): {svm1_test_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for test data set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_test, svm1_test_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, svm1_test_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082651f4",
   "metadata": {},
   "source": [
    "### Train and test with \"linear\" kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear classifier and train with the training set.\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "svm2 = SVC(kernel='linear', random_state=0)\n",
    "print(svm2.get_params())\n",
    "\n",
    "# Time how long it takes to train.\n",
    "start = time.time()\n",
    "svm2.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "svm2_train_time = end - start\n",
    "\n",
    "print(f\"secs to fit: {svm2_train_time:.2f}\")\n",
    "print(f\"iterations required: {svm2.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a267c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm2_train_predict = svm2.predict(X_train)\n",
    "end = time.time()\n",
    "svm2_pred_time = end - start\n",
    "\n",
    "print(f\"secs to predict (training set): {svm2_pred_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for training data set\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_train, svm2_train_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, svm2_train_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73336f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm2_test_predict = svm2.predict(X_test)\n",
    "end = time.time()\n",
    "svm2_test_time = end - start\n",
    "\n",
    "print(f\"secs to predict (test set): {svm2_test_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for test data set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_test, svm2_test_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, svm2_test_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fc55a",
   "metadata": {},
   "source": [
    "### Train and test with \"poly\" kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2052f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Poly classifier and train with the training set.\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "svm3 = SVC(kernel='poly', random_state=0)\n",
    "print(svm3.get_params())\n",
    "\n",
    "# Time how long it takes to train.\n",
    "start = time.time()\n",
    "svm3.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "svm3_train_time = end - start\n",
    "\n",
    "print(f\"secs to fit: {svm3_train_time:.2f}\")\n",
    "print(f\"iterations required: {svm3.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd24c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm3_train_predict = svm3.predict(X_train)\n",
    "end = time.time()\n",
    "svm3_pred_time = end - start\n",
    "\n",
    "print(f\"secs to predict (training set): {svm3_pred_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for training data set\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_train, svm3_train_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, svm3_train_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c371ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm3_test_predict = svm3.predict(X_test)\n",
    "end = time.time()\n",
    "svm3_test_time = end - start\n",
    "\n",
    "print(f\"secs to predict (test set): {svm3_test_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08825acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for test data set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_test, svm3_test_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, svm3_test_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0463e8",
   "metadata": {},
   "source": [
    "### Train and test with \"sigmoid\" kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f562d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Poly classifier and train with the training set.\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "svm4 = SVC(kernel='sigmoid', random_state=0)\n",
    "print(svm4.get_params())\n",
    "\n",
    "# Time how long it takes to train.\n",
    "start = time.time()\n",
    "svm4.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "svm4_train_time = end - start\n",
    "\n",
    "print(f\"secs to fit: {svm4_train_time:.2f}\")\n",
    "print(f\"iterations required: {svm4.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm4_train_predict = svm4.predict(X_train)\n",
    "end = time.time()\n",
    "svm4_pred_time = end - start\n",
    "\n",
    "print(f\"secs to predict (training set): {svm4_pred_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c978888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for training data set\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_train, svm4_train_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, svm4_train_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set. Time how long it takes to predict.\n",
    "start = time.time()\n",
    "svm4_test_predict = svm4.predict(X_test)\n",
    "end = time.time()\n",
    "svm4_test_time = end - start\n",
    "\n",
    "print(f\"secs to predict (test set): {svm4_test_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and ConfusionMatrix for test data set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_labels = [0, 1]\n",
    "target_names = ['nondos', 'dos']\n",
    "print(classification_report(y_test, svm4_test_predict, labels=target_labels, target_names=target_names, digits=4))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, svm4_test_predict, values_format=\",\", display_labels = target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e850e2",
   "metadata": {},
   "source": [
    "### Compare Kernels\n",
    "\n",
    "Below we compare the results of the different kernels. We see that Poly and RBF had the best accuracy, with linear slightly lower and sigmoid much lower. Sigmoid and Poly took much more time to train than the other two; linear required a lot of iterations to train, but the time needed was not as much as Sigmoid or Poly. Surprisingly, though Poly took a long time to train, it has the fastest testing time of any of the kernels.\n",
    "\n",
    "Overall, RBF gives the best results for this data set, both in computation requirements to train and in accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d981bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scores array for all the different SVM kernels\n",
    "scores = []\n",
    "scores.append(svm1.score(X_test, y_test))\n",
    "scores.append(svm2.score(X_test, y_test))\n",
    "scores.append(svm3.score(X_test, y_test))\n",
    "scores.append(svm4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores (mean accuracy) for each SVM kernel type\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "ax.barh(kernels,scores)\n",
    "\n",
    "max_xlim = max(scores) + np.std(scores)\n",
    "min_xlim = min(scores) - np.std(scores)\n",
    "plt.xlim(min_xlim, max_xlim)\n",
    "plt.xlabel('Mean Accuracy')\n",
    "plt.ylabel('SVM Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08071931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train_times array\n",
    "train_times = []\n",
    "train_times.append(svm1_train_time)\n",
    "train_times.append(svm2_train_time)\n",
    "train_times.append(svm3_train_time)\n",
    "train_times.append(svm4_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "ax.barh(kernels,train_times)\n",
    "\n",
    "max_xlim = max(train_times) + np.std(train_times)\n",
    "min_xlim = 0\n",
    "plt.xlim(min_xlim, max_xlim)\n",
    "plt.xlabel('Time to Train (secs)')\n",
    "plt.ylabel('SVM Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate number of iterations array\n",
    "num_iter = []\n",
    "num_iter.append(svm1.n_iter_[0])\n",
    "num_iter.append(svm2.n_iter_[0])\n",
    "num_iter.append(svm3.n_iter_[0])\n",
    "num_iter.append(svm4.n_iter_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "ax.barh(kernels,num_iter)\n",
    "\n",
    "max_xlim = max(num_iter) + np.std(num_iter)\n",
    "min_xlim = 0\n",
    "plt.xlim(min_xlim, max_xlim)\n",
    "plt.xlabel('Number of Iterations Needed to Train')\n",
    "plt.ylabel('SVM Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test_times array\n",
    "test_times = []\n",
    "test_times.append(svm1_test_time)\n",
    "test_times.append(svm2_test_time)\n",
    "test_times.append(svm3_test_time)\n",
    "test_times.append(svm4_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8987c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "ax.barh(kernels,test_times)\n",
    "\n",
    "max_xlim = max(test_times) + np.std(test_times)\n",
    "min_xlim = 0\n",
    "plt.xlim(min_xlim, max_xlim)\n",
    "plt.xlabel('Time to Test (secs)')\n",
    "plt.ylabel('SVM Kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab6dac",
   "metadata": {},
   "source": [
    "## Task 3: Select 2 Features. Train and test the Linear and RBF kernels against the 2 feature set, and visualize the decision bounary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b4ca1",
   "metadata": {},
   "source": [
    "### Select Top 2 Features via Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "\n",
    "print(\"selecting best via chi2:\")\n",
    "selector = SelectKBest(chi2, k=2).fit(X_reduced, y1)\n",
    "print(selector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f14fde",
   "metadata": {},
   "source": [
    "### Find important features using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LogisticRegression model with the training data.\n",
    "# After trial and error, 300 is roughly the minimum number of iterations at which the training will converge.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0, max_iter=5000)\n",
    "print(lr.get_params())\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"iterations required: {lr.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of each feature with its coeffiecient, in sorted order\n",
    "numFeatures = lr.coef_[0].size\n",
    "sorted_coef = np.empty(shape = (numFeatures,2), dtype=object)\n",
    "for i in range(numFeatures):\n",
    "    sorted_coef[i, 0] = i\n",
    "    sorted_coef[i, 1] = lr.coef_[0][i]\n",
    "\n",
    "sorted_coef = sorted_coef[sorted_coef[:,1].argsort()] \n",
    "\n",
    "print(\"Most positively correlated features:\")    \n",
    "for rank in range (14,-1,-1): \n",
    "    colnum, coef = sorted_coef[rank]\n",
    "    print(f\"{X_reduced.columns[colnum]} ({colnum}): {coef:6.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6c5ed",
   "metadata": {},
   "source": [
    "## Train using 2 features\n",
    "\n",
    "We decide to use \"count\" and \"srv_count\" for our features. \"srv_count\" has the largest absolute value coefficient from our LogisticRegression model, and \"count\" is picked as one of the two most important features by the SelectKBest class. More importantly, those columns have a more even data spread than most columns, and so gives us a more interesting data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91eebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the feature set and result set from the data\n",
    "print(f\"shape of data set: {df.shape}\")\n",
    "print(X.columns)\n",
    "\n",
    "# Include only count and srv_count in the new data set (X20)\n",
    "#X20 = X.iloc[:, [23, 35]]\n",
    "#X20 = X.iloc[:, [4, 5]]\n",
    "X20 = X_reduced.iloc[:, [1, 2]] # count, srv_count\n",
    "\n",
    "print(f\"shape of X: {X20.shape}\")\n",
    "print(X20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115944b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to only 50 positive and 50 negative records, to make visualization reasonable.\n",
    "# Our new data set we call X21.\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "print('Original dataset shape %s' % Counter(y1))\n",
    "rus = RandomUnderSampler(random_state=20, sampling_strategy={1:50, 0:50})\n",
    "X21, y21 = rus.fit_resample(X20, y1)\n",
    "print('Resampled dataset shape %s' % Counter(y21))\n",
    "print(X21.shape)\n",
    "X21.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba64501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our plotting mechanisms want ndarray, so convert from Dataframe to ndarray.\n",
    "X22 = X21.values\n",
    "y22 = y21\n",
    "\n",
    "print(X22.shape)\n",
    "print(y22.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for plotting the data set\n",
    "pos_feat1 = []\n",
    "pos_feat2 = []\n",
    "neg_feat1 = []\n",
    "neg_feat2 = []\n",
    "\n",
    "for i in range(y22.size):\n",
    "    if y22[i] == 1:\n",
    "        pos_feat1.append(X22[i][0])\n",
    "        pos_feat2.append(X22[i][1])\n",
    "    else:\n",
    "        neg_feat1.append(X22[i][0])\n",
    "        neg_feat2.append(X22[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data set, using Red X's for non-DOS and Blue Circles for DOS\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot data\n",
    "plt.scatter(neg_feat1, neg_feat2,\n",
    "            color='red', marker='x', label='nondos')\n",
    "plt.scatter(pos_feat1, pos_feat2,\n",
    "            color='blue', marker='o', label='dos')\n",
    "\n",
    "plt.xlabel(X21.columns[0])\n",
    "plt.ylabel(X21.columns[1])\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81c97",
   "metadata": {},
   "source": [
    "## Train against linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm5 = SVC(kernel='linear', random_state=0)\n",
    "print(svm5.get_params())\n",
    "svm5.fit(X22, y22)\n",
    "\n",
    "print(f\"iterations required: {svm5.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot_decision_regions function, which is copied from Week 2 lab\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', 'x', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2aa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision region generated by the Linear SVM kernel\n",
    "plot_decision_regions(X22, y22, classifier=svm5)\n",
    "plt.title('Linear Kernel')\n",
    "plt.xlabel(X21.columns[0])\n",
    "plt.ylabel(X21.columns[1])\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31ebd4",
   "metadata": {},
   "source": [
    "## Train against RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559034e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm6 = SVC(kernel='rbf', random_state=0)\n",
    "print(svm6.get_params())\n",
    "svm6.fit(X22, y22)\n",
    "\n",
    "print(f\"iterations required: {svm6.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision region generated by the RBF SVM Kernel\n",
    "plot_decision_regions(X22, y22, classifier=svm6)\n",
    "plt.title('RBF Kernel')\n",
    "plt.xlabel('srv_count')\n",
    "plt.ylabel('dst_host_same_src_port_rate')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a7831",
   "metadata": {},
   "source": [
    "### Discuss our observations\n",
    "\n",
    "As indicated by the name, the linear kernel created a straight-line boundary for the decision regions. The decision boundary does not work very well for this very small, 2 feature data set. The RBF kernel, on the other hand, drew a shape for the decision boundary that looks like a slightly distorted circle. The boundary is very interesting and captures the data sets much better than the linear kernel did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd6efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
